{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiTIpPjArIyr"
   },
   "source": [
    "# Full example of music generation, with the Hugging Face GPT2 Transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOd93yV0sGd2"
   },
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "fX12Yquyuihc"
   },
   "outputs": [],
   "source": [
    "#@title Install all dependencies (run only once per session)\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "!pip install miditok\n",
    "!pip install miditoolkit\n",
    "!pip install torch\n",
    "!pip install torchtoolkit\n",
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "\n",
    "!wget http://www-ens.iro.umontreal.ca/~boulanni/JSB%20Chorales.zip\n",
    "!unzip 'JSB Chorales.zip'\n",
    "!rm 'JSB Chorales.zip'\n",
    "!mv 'JSB Chorales' 'JSB'\n",
    "\n",
    "from typing import List, Tuple, Callable\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import json\n",
    "\n",
    "from torch import LongTensor, cat, stack, full, flip\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.nn import Module, CrossEntropyLoss\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtoolkit.train import train, log_model_parameters, log_cuda_info, select_device\n",
    "from torchtoolkit.data import create_subsets\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from miditok import REMI\n",
    "from miditoolkit import MidiFile\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert MIDI files to tokens, and load them for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our parameters\n",
    "pitch_range = range(21, 109)\n",
    "beat_res = {(0, 4): 8, (4, 12): 4}\n",
    "nb_velocities = 32\n",
    "additional_tokens = {'Chord': True, 'Rest': True, 'Tempo': True,\n",
    "                     'rest_range': (2, 8),  # (half, 8 beats)\n",
    "                     'nb_tempos': 32,  # nb of tempo bins\n",
    "                     'tempo_range': (40, 250),  # (min, max)\n",
    "                     'Program': False}\n",
    "\n",
    "# Creates the tokenizer convert MIDIs to tokens\n",
    "tokens_path = Path('JSB_tokens')\n",
    "tokenizer = REMI(pitch_range, beat_res, nb_velocities, additional_tokens, sos_eos=True) # REMI encoding\n",
    "midi_paths = list(Path('JSB').glob('**/*.mid'))\n",
    "tokenizer.tokenize_midi_dataset(midi_paths, tokens_path)\n",
    "\n",
    "class MIDIDataset:\n",
    "    r\"\"\"Dataset for generator training\n",
    "\n",
    "    :param data_path: path containing the real data to load, ex: 'data/death_metal_dataset'.\n",
    "    :param min_seq_len: minimum sequence length (in nb of tokens)\n",
    "    :param max_seq_len: maximum sequence length (in nb of tokens)\n",
    "    :param padding_token: padding token, usually 0.\n",
    "    :param sos_token: \"Start Of Sequence\" token, to be placed at the beginning of each token sequence.\n",
    "    :param tokenizer: tokenizer object, to use when fake_data_path is a list of MIDI paths. (default: None)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path: Path, min_seq_len: int, max_seq_len: int, padding_token: int, sos_token: int,\n",
    "                 tokenizer = None):\n",
    "        self.pad_token = padding_token\n",
    "        self.sos_token = sos_token\n",
    "        samples = []\n",
    "        as_midi = False\n",
    "        files_paths = list(Path(data_path).glob(f'**/*.json'))\n",
    "        if len(files_paths) == 0:\n",
    "            files_paths = list(Path(data_path).glob(f'**/*.mid'))\n",
    "            as_midi = True\n",
    "\n",
    "        for file_path in tqdm(files_paths, desc=f'Preparing data {data_path.name}'):\n",
    "            if as_midi:\n",
    "                tokens = tokenizer.midi_to_tokens(MidiFile(file_path))[0]  # first track\n",
    "            else:\n",
    "                with open(file_path) as json_file:\n",
    "                    tokens = json.load(json_file)['tokens'][0]  # first track\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i >= len(tokens) - min_seq_len:\n",
    "                    break  # last sample is too short\n",
    "                samples.append(LongTensor(tokens[i:i + max_seq_len]))\n",
    "                i += len(samples[-1])  # could be replaced with max_seq_len\n",
    "\n",
    "        self.samples = samples\n",
    "    \n",
    "    def collate_fn(self, batch: List[LongTensor]) -> Tuple[LongTensor, LongTensor]:\n",
    "        batch = pad_sequence(batch, batch_first=True, padding_value=self.pad_token)  # (N,T) or (N,T,Z)\n",
    "        (sos_shape := list(batch.shape))[1] = 1  # (N,1) or (N,1,Z)\n",
    "        batch = cat([full(sos_shape, self.sos_token), batch], dim=1)  # adds sos token to every samples\n",
    "        return batch[:, :-1], batch[:, 1:]\n",
    "\n",
    "    def collate_fn_infer(self, batch: List[LongTensor]) -> LongTensor:\n",
    "        # Here the sequences are padded to the left, so that the last element along the time dimension\n",
    "        # is always the last of each seq, allowing to efficiently generate by batch\n",
    "        sos_shape = (1,) if batch[0].dim() == 1 else (1, batch[0].shape[-1])  # (1) or (1,Z)\n",
    "        batch = [flip(cat([full(sos_shape, self.sos_token), seq], dim=0), dims=(0, )) for seq in batch]\n",
    "        batch = pad_sequence(batch, batch_first=True, padding_value=self.pad_token)  # (N,T) or (N,T,Z)\n",
    "        batch = flip(batch, dims=(1, )).long()\n",
    "        return batch\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[LongTensor, int]: return self.samples[idx]\n",
    "    \n",
    "    def __len__(self) -> int: return len(self.samples)\n",
    "\n",
    "    def __repr__(self): return self.__str__()\n",
    "\n",
    "    def __str__(self) -> str: return 'No data loaded' if len(self) == 0 else f'{len(self.samples)} samples'\n",
    "\n",
    "\n",
    "# Loads tokens and create data loaders for training\n",
    "dataset = MIDIDataset(\n",
    "    tokens_path, max_seq_len=512, min_seq_len=384, \n",
    "    padding_token=tokenizer['PAD_None'],\n",
    "    sos_token=tokenizer['SOS_None']\n",
    ")\n",
    "subset_train, subset_valid = create_subsets(dataset, [0.3])\n",
    "dataloader_train = DataLoader(subset_train, batch_size=16, collate_fn=dataset.collate_fn)\n",
    "dataloader_valid = DataLoader(subset_valid, batch_size=16, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "We will use the [GPT2 implementation of Hugging Face](https://huggingface.co/docs/transformers/model_doc/gpt2). This \n",
    "Feel free to explore the documentation and source code to dig deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(GPT2LMHeadModel):\n",
    "    def __init__(self, config: GPT2Config, padding_token: int):\n",
    "        super().__init__(config)\n",
    "        self.transformer.wpe.padding_idx = padding_token  # updates the padding idx\n",
    "        self.transformer.wte.padding_idx = padding_token\n",
    "\n",
    "    def forward_train(self, x: LongTensor, target: LongTensor, criterion: Module):\n",
    "        y = self.forward(x).logits  # (N,T,C)\n",
    "        loss = criterion(y.transpose(2, 1), target)\n",
    "        return y, loss, None  # no need for sampled\n",
    "\n",
    "\n",
    "# Creates model\n",
    "config = GPT2Config(vocab_size=len(tokenizer), n_positions=2048, n_embd=512, n_layer=8, n_head=8,\n",
    "                    n_inner=2048, resid_pdrop=.1, embd_pdrop=.1, attn_pdrop=.1,\n",
    "                    padding_token_id=tokenizer['PAD_None'], bos_token_id=tokenizer['SOS_None'],\n",
    "                    eos_token_id=tokenizer['EOS_None'])\n",
    "model = Transformer(config, padding_token=tokenizer['PAD_None'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path('run')\n",
    "device = select_device(True)\n",
    "model = model.to(device)\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(params=model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "lr_scheduler = CosineAnnealingWarmRestarts(optimizer, 20, 2)\n",
    "\n",
    "log_model_parameters(model)\n",
    "if device.type == 'cuda':\n",
    "      log_cuda_info()\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    dataloader_train=dataloader_train,\n",
    "    dataloader_valid=dataloader_valid,\n",
    "    nb_steps=50000,\n",
    "    valid_intvl=20,\n",
    "    nb_valid_steps=10,\n",
    "    log_intvl=100,\n",
    "    pbar_desc='MIDI generator training',\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    use_amp=True,\n",
    "    gradient_clip_norm=3,\n",
    "    device=device,\n",
    "    saving_dir=save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OaNkGcFo9UP_"
   },
   "outputs": [],
   "source": [
    "nb_inferences = 512  # extends samples by 512 tokens\n",
    "(gen_results_path := Path('gen_res')).mkdir(parents=True, exist_ok=True)\n",
    "dataloader_test = DataLoader(subset_valid, batch_size=16, collate_fn=dataset.collate_fn_infer)\n",
    "\n",
    "model.eval()\n",
    "count = 0\n",
    "for batch in tqdm(dataloader_test, desc='Testing model / Generating results'):  # (N,T)\n",
    "    # Attention mask (handling padding), sampling are handled in generate method\n",
    "    if device.type == 'cuda':\n",
    "        batch = batch.to(device)\n",
    "    res = model.generate(batch, do_sample=True, num_beams=5, top_p=0.9, max_new_tokens=600)  # (N,T)\n",
    "\n",
    "    # Saves the generated music, as MIDI files and tokens (json)\n",
    "    for prompt, continuation in zip(batch, res):\n",
    "\n",
    "        generated = continuation[len(prompt):]\n",
    "        tokens = [generated, prompt, continuation]  # list compr. as seqs of dif. lengths\n",
    "        tokens = [seq.tolist() for seq in tokens]\n",
    "        midi = tokenizer.tokens_to_midi(deepcopy(tokens), time_division=384)\n",
    "        midi.instruments[0].name = f'Continuation of original sample ({len(generated)} tokens)'\n",
    "        midi.instruments[1].name = f'Original sample ({len(prompt)} tokens)'\n",
    "        midi.instruments[2].name = f'Original sample and continuation'\n",
    "        midi.dump(gen_results_path / f'{count}.mid')\n",
    "        tokenizer.save_tokens(tokens, gen_results_path / f'{count}.json')   \n",
    "\n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Optimus_VIRTUOSO_Multi_Instrumental_RGA_Edition.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
