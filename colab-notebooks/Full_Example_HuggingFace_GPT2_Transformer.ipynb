{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SiTIpPjArIyr"
   },
   "source": [
    "# Full example with the Hugging Face Transformers package\n",
    "\n",
    "This notebook shows how to train a model (GPT2) and generate music from it, using the Hugging Face Transformers package."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gOd93yV0sGd2"
   },
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "fX12Yquyuihc"
   },
   "outputs": [],
   "source": [
    "#@title Install all dependencies (run only once per session)\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "!pip install miditok\n",
    "!pip install miditoolkit\n",
    "!pip install torch\n",
    "!pip install torchtoolkit\n",
    "!pip install transformers\n",
    "!pip install evaluate\n",
    "!pip install tqdm\n",
    "\n",
    "!wget https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0-midi.zip\n",
    "!unzip 'maestro-v3.0.0-midi.zip'\n",
    "!rm 'maestro-v3.0.0-midi.zip'\n",
    "!mv 'maestro-v3.0.0' 'Maestro'\n",
    "\n",
    "from typing import List, Tuple, Dict, Callable, Any, Union\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import json\n",
    "\n",
    "from torch import Tensor, LongTensor, stack, flip, cat, full, argmax\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtoolkit.data import create_subsets\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, Trainer, TrainingArguments, GenerationConfig\n",
    "from transformers.data.data_collator import DataCollatorMixin\n",
    "from evaluate import load as load_metric\n",
    "from miditok import REMI, MIDITokenizer, TokenizerConfig\n",
    "from miditok.constants import CHORD_MAPS\n",
    "from miditoolkit import MidiFile\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class MIDIDataset(Dataset):\n",
    "    r\"\"\"Dataset for generator training\n",
    "\n",
    "    :param files_paths: list of paths to files to load.\n",
    "    :param tokenizer: tokenizer object, to use to load MIDIs instead of tokens. (default: None)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, files_paths: List[Path], min_seq_len: int, max_seq_len: int, tokenizer: MIDITokenizer = None):\n",
    "        samples = []\n",
    "\n",
    "        for file_path in tqdm(files_paths, desc=f'Loading data: {files_paths[0].parent}'):\n",
    "            if file_path.suffix in [\"mid\", \"midi\", \"MID\", \"MIDI\"]:\n",
    "                midi = MidiFile(file_path)\n",
    "                for _ in range(len(midi.instruments) - 1):\n",
    "                    del midi.instruments[1]  # removes all tracks except first one\n",
    "                tokens = tokenizer.midi_to_tokens(midi)[0].ids\n",
    "            else:\n",
    "                with open(file_path) as json_file:\n",
    "                    tokens = json.load(json_file)['ids'][0]  # first track\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i >= len(tokens) - min_seq_len:\n",
    "                    break  # last sample is too short\n",
    "                samples.append(LongTensor(tokens[i:i + max_seq_len]))\n",
    "                i += len(samples[-1])  # could be replaced with max_seq_len\n",
    "\n",
    "        self.samples = samples\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict[str, LongTensor]:\n",
    "        return {\"input_ids\": self.samples[idx], \"labels\": self.samples[idx]}\n",
    "    \n",
    "    def __len__(self) -> int: return len(self.samples)\n",
    "\n",
    "    def __repr__(self): return self.__str__()\n",
    "\n",
    "    def __str__(self) -> str: return 'No data loaded' if len(self) == 0 else f'{len(self.samples)} samples'\n",
    "\n",
    "\n",
    "def _pad_batch(examples: List[Dict[str, LongTensor]], pad_token: int) -> LongTensor:\n",
    "    \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
    "\n",
    "    length_of_first = examples[0][\"input_ids\"].size(0)\n",
    "\n",
    "    # Check if padding is necessary.\n",
    "    are_tensors_same_length = all(x[\"input_ids\"].size(0) == length_of_first for x in examples)\n",
    "    if are_tensors_same_length:\n",
    "        return stack([e[\"input_ids\"] for e in examples], dim=0).long()\n",
    "\n",
    "    # Creating the full tensor and filling it with our data.\n",
    "    return pad_sequence([e[\"input_ids\"] for e in examples], batch_first=True, padding_value=pad_token).long()\n",
    "\n",
    "\n",
    "class DataCollatorGen(DataCollatorMixin):\n",
    "    def __init__(self, pad_token: int, return_tensors: str = \"pt\"):\n",
    "        \"\"\"Collator that simply pad the input sequences.\n",
    "        Input_ids will be padded with the pad token given, while labels will be\n",
    "        padded with -100.\n",
    "\n",
    "        :param pad_token: pas token\n",
    "        :param return_tensors:\n",
    "        \"\"\"\n",
    "        self.pad_token = pad_token\n",
    "        self.return_tensors = return_tensors\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]], return_tensors=None) -> Dict[str, LongTensor]:\n",
    "        x, y = _pad_batch(batch, self.pad_token), _pad_batch(batch, -100)\n",
    "        return {\"input_ids\": x, \"labels\": y}  # will be shifted in GPT2LMHead forward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert MIDI files to tokens, and load them for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our tokenizer's configuration\n",
    "PITCH_RANGE = range(21, 109)\n",
    "BEAT_RES = {(0, 1): 8, (1, 2): 4, (2, 4): 2, (4, 8): 1}\n",
    "NB_VELOCITIES = 24\n",
    "SPECIAL_TOKENS = [\"PAD\", \"MASK\", \"BOS\", \"EOS\"]\n",
    "USE_CHORDS = False\n",
    "USE_RESTS = False\n",
    "USE_TEMPOS = True\n",
    "USE_TIME_SIGNATURE = False\n",
    "USE_PROGRAMS = True\n",
    "NB_TEMPOS = 32\n",
    "TEMPO_RANGE = (50, 200)  # (min_tempo, max_tempo)\n",
    "TOKENIZER_PARAMS = {\n",
    "    \"pitch_range\": PITCH_RANGE,\n",
    "    \"beat_res\": BEAT_RES,\n",
    "    \"nb_velocities\": NB_VELOCITIES,\n",
    "    \"special_tokens\": SPECIAL_TOKENS,\n",
    "    \"use_chords\": USE_CHORDS,\n",
    "    \"use_rests\": USE_RESTS,\n",
    "    \"use_tempos\": USE_TEMPOS,\n",
    "    \"use_time_signatures\": USE_TIME_SIGNATURE,\n",
    "    \"use_programs\": USE_PROGRAMS,\n",
    "    \"nb_tempos\": NB_TEMPOS,\n",
    "    \"tempo_range\": TEMPO_RANGE,\n",
    "}\n",
    "config = TokenizerConfig(**TOKENIZER_PARAMS)\n",
    "\n",
    "# Creates the tokenizer convert MIDIs to tokens\n",
    "tokens_path = Path('Maestro_tokens_no_bpe')\n",
    "tokenizer = REMI(config)  # REMI\n",
    "midi_paths = list(Path('Maestro').glob('**/*.mid')) + list(Path('Maestro').glob('**/*.midi'))\n",
    "tokenizer.tokenize_midi_dataset(midi_paths, tokens_path)\n",
    "\n",
    "# Learn and apply BPE to data we just tokenized\n",
    "tokens_bpe_path = Path('Maestro_tokens_bpe')\n",
    "tokens_bpe_path.mkdir(exist_ok=True, parents=True)\n",
    "tokenizer.learn_bpe(\n",
    "    vocab_size=10000,\n",
    "    tokens_paths=list(tokens_path.glob(\"**/*.json\")),\n",
    "    start_from_empty_voc=False,\n",
    ")\n",
    "tokenizer.apply_bpe_to_dataset(\n",
    "    tokens_path,\n",
    "    tokens_bpe_path,\n",
    ")\n",
    "\n",
    "# Loads tokens and create data loaders for training\n",
    "tokens_paths = list(Path('Maestro_tokens_bpe').glob(\"**/*.json\"))\n",
    "dataset = MIDIDataset(\n",
    "    tokens_paths, max_seq_len=512, min_seq_len=384, \n",
    ")\n",
    "subset_train, subset_valid = create_subsets(dataset, [0.3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "We will use the [GPT2 implementation of Hugging Face](https://huggingface.co/docs/transformers/model_doc/gpt2). This \n",
    "Feel free to explore the documentation and source code to dig deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates model\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_positions=2048,\n",
    "    n_embd=512,\n",
    "    n_layer=8,\n",
    "    n_head=8,\n",
    "    n_inner=2048,\n",
    "    resid_pdrop=.1,\n",
    "    embd_pdrop=.1,\n",
    "    attn_pdrop=.1,\n",
    "    padding_token_id=tokenizer['PAD_None'],\n",
    "    bos_token_id=tokenizer['BOS_None'],\n",
    "    eos_token_id=tokenizer['EOS_None'],\n",
    ")\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {metric: load_metric(metric) for metric in [\"accuracy\"]}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes metrics for pretraining.\n",
    "    Must use proprocess_logits function that converts logits to predictions (argmax or sampling).\n",
    "\n",
    "    :param eval_pred: EvalPrediction containing predictions and labels\n",
    "    :return: metrics\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    not_pad_mask = labels != -100\n",
    "    labels, predictions = labels[not_pad_mask], predictions[not_pad_mask]\n",
    "    return metrics[\"accuracy\"].compute(predictions=predictions.flatten(), references=labels.flatten())\n",
    "\n",
    "def preprocess_logits(logits: Tensor, _: Tensor) -> Tensor:\n",
    "    \"\"\"Preprocesses the logits before accumulating them during evaluation.\n",
    "    This allows to significantly reduce the memory usage and make the training tractable.\n",
    "    \"\"\"\n",
    "    pred_ids = argmax(logits, dim=-1)  # long dtype\n",
    "    return pred_ids\n",
    "\n",
    "training_config = TrainingArguments(\n",
    "    \"runs\", False, True, True, False, \"steps\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=48,\n",
    "    gradient_accumulation_steps=3,\n",
    "    eval_accumulation_steps=None,\n",
    "    eval_steps=1000,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=3.0,\n",
    "    max_steps=100000,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    warmup_ratio=0.3,\n",
    "    log_level=\"debug\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=5,\n",
    "    no_cuda=False,\n",
    "    seed=444,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    label_smoothing_factor=0.,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=[\"tensorboard\"],\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_config,\n",
    "    data_collator=DataCollatorGen(tokenizer[\"PAD_None\"]),\n",
    "    train_dataset=subset_train,\n",
    "    eval_dataset=subset_valid,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=None,\n",
    "    preprocess_logits_for_metrics=preprocess_logits,\n",
    ")\n",
    "\n",
    "# Training\n",
    "train_result = trainer.train()\n",
    "trainer.save_model()  # Saves the tokenizer too\n",
    "trainer.log_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OaNkGcFo9UP_"
   },
   "outputs": [],
   "source": [
    "def collate_gen_left(batch: List[Dict[str, LongTensor]]) -> LongTensor:\n",
    "    # Here the sequences are padded to the left, so that the last token along the time dimension\n",
    "    # is always the last token of each seq, allowing to efficiently generate by batch\n",
    "    bos_shape = (1,)\n",
    "    batch = [flip(cat([full(bos_shape, tokenizer[\"BOS_None\"]), seq[\"input_ids\"]], dim=0), dims=(0,)) for seq in batch]\n",
    "    batch = pad_sequence(batch, batch_first=True, padding_value=tokenizer[\"PAD_None\"])  # (N,T) or (N,T,Z)\n",
    "    batch = flip(batch, dims=(1,)).long()\n",
    "    return batch  # (N,T)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=512,  # extends samples by 512 tokens\n",
    "    num_beams=1,        # no beam search\n",
    "    do_sample=True,     # but sample instead\n",
    "    temperature=0.9,\n",
    "    top_k=15,\n",
    "    top_p=0.95,\n",
    "    epsilon_cutoff=3e-4,\n",
    "    eta_cutoff=1e-3,\n",
    "    pad_token_id=config.padding_token_id,\n",
    ")\n",
    "\n",
    "(gen_results_path := Path('gen_res')).mkdir(parents=True, exist_ok=True)\n",
    "dataloader_test = DataLoader(subset_valid, batch_size=16, collate_fn=collate_gen_left)\n",
    "model.eval()\n",
    "count = 0\n",
    "for batch in tqdm(dataloader_test, desc='Testing model / Generating results'):  # (N,T)\n",
    "    res = model.generate(batch.to(model.device), generation_config=generation_config)  # (N,T)\n",
    "\n",
    "    # Saves the generated music, as MIDI files and tokens (json)\n",
    "    for prompt, continuation in zip(batch, res):\n",
    "        generated = continuation[len(prompt):]\n",
    "        tokens = [generated, prompt, continuation]  # list compr. as seqs of dif. lengths\n",
    "        tokens = [seq.tolist() for seq in tokens]\n",
    "        midi = tokenizer.tokens_to_midi(deepcopy(tokens), time_division=384)\n",
    "        midi.instruments[0].name = f'Continuation of original sample ({len(generated)} tokens)'\n",
    "        midi.instruments[1].name = f'Original sample ({len(prompt)} tokens)'\n",
    "        midi.instruments[2].name = f'Original sample and continuation'\n",
    "        midi.dump(gen_results_path / f'{count}.mid')\n",
    "        tokenizer.save_tokens(tokens, gen_results_path / f'{count}.json')   \n",
    "\n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Optimus_VIRTUOSO_Multi_Instrumental_RGA_Edition.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
