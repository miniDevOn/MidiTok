{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiTIpPjArIyr"
   },
   "source": [
    "# Full example / tutorial of training a Transformer model (GPT2) for symbolic music generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOd93yV0sGd2"
   },
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "fX12Yquyuihc"
   },
   "outputs": [],
   "source": [
    "#@title Install all dependencies (run only once per session)\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "!pip install miditok\n",
    "!pip install miditoolkit\n",
    "!pip install torch\n",
    "!pip install torchtoolkit\n",
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "\n",
    "!wget http://www-ens.iro.umontreal.ca/~boulanni/JSB%20Chorales.zip\n",
    "!unzip 'JSB Chorales.zip'\n",
    "!rm 'JSB Chorales.zip'\n",
    "!mv 'JSB Chorales' 'JSB'\n",
    "\n",
    "from typing import List, Tuple, Callable\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import json\n",
    "\n",
    "from torch import LongTensor, Tensor, cat, stack, no_grad, no_grad\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.nn import Module, CrossEntropyLoss\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtoolkit.sampling import nucleus\n",
    "from torchtoolkit.train import train, log_model_parameters, log_cuda_info, select_device\n",
    "from torchtoolkit.data import create_subsets\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from miditok import REMI\n",
    "from miditoolkit import MidiFile\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converts MIDI files to tokens, and load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our parameters\n",
    "pitch_range = range(21, 109)\n",
    "beat_res = {(0, 4): 8, (4, 12): 4}\n",
    "nb_velocities = 32\n",
    "additional_tokens = {'Chord': True, 'Rest': True, 'Tempo': True,\n",
    "                     'rest_range': (2, 8),  # (half, 8 beats)\n",
    "                     'nb_tempos': 32,  # nb of tempo bins\n",
    "                     'tempo_range': (40, 250),  # (min, max)\n",
    "                     'Program': False}\n",
    "\n",
    "# Creates the tokenizer convert MIDIs to tokens\n",
    "tokens_path = Path('JSB_tokens')\n",
    "tokenizer = REMI(pitch_range, beat_res, nb_velocities, additional_tokens) # REMI encoding\n",
    "midi_paths = list(Path('JSB').glob('**/*.mid'))\n",
    "tokenizer.tokenize_midi_dataset(midi_paths, tokens_path)\n",
    "\n",
    "class MIDIDataset:\n",
    "    r\"\"\"Dataset for generator training\n",
    "\n",
    "    :param data_path: path containing the real data to load, ex: 'data/death_metal_dataset'.\n",
    "    :param min_seq_len: minimum sequence length (in nb of tokens)\n",
    "    :param max_seq_len: maximum sequence length (in nb of tokens)\n",
    "    :param padding_token: padding token, usually 0.\n",
    "    :param tokenizer: tokenizer object, to use when fake_data_path is a list of MIDI paths. (default: None)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path: Path, min_seq_len: int, max_seq_len: int, padding_token: int,\n",
    "                 tokenizer = None):\n",
    "        samples = []\n",
    "        as_midi = False\n",
    "        files_paths = list(Path(data_path).glob(f'**/*.json'))\n",
    "        if len(files_paths) == 0:\n",
    "            files_paths = list(Path(data_path).glob(f'**/*.mid'))\n",
    "            as_midi = True\n",
    "\n",
    "        for file_path in tqdm(files_paths, desc=f'Preparing data {data_path.name}'):\n",
    "            if as_midi:\n",
    "                tokens = tokenizer.midi_to_tokens(MidiFile(file_path))[0]  # first track\n",
    "            else:\n",
    "                with open(file_path) as json_file:\n",
    "                    tokens = json.load(json_file)['tokens'][0]  # first track\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i >= len(tokens) - min_seq_len:\n",
    "                    break  # last sample is too short\n",
    "                samples.append(LongTensor(tokens[i:i + max_seq_len]))\n",
    "                i += len(samples[-1])  # could be replaced with max_seq_len\n",
    "\n",
    "        self.samples = pad_sequence(samples, batch_first=True, padding_value=padding_token)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[LongTensor, int]: return self.samples[idx]\n",
    "    \n",
    "    def __len__(self) -> int: return len(self.samples)\n",
    "\n",
    "    def __repr__(self): return self.__str__()\n",
    "\n",
    "    def __str__(self) -> str: return 'No data loaded' if len(self) == 0 else f'{len(self.samples)} samples'\n",
    "\n",
    "\n",
    "def collate_gen(batch: List[LongTensor]) -> Tuple[LongTensor, LongTensor]:\n",
    "    batch = stack(batch)  # (N,T)\n",
    "    return batch[:, :-1], batch[:, 1:]\n",
    "\n",
    "\n",
    "# Loads tokens and create data loaders for training\n",
    "dataset = MIDIDataset(tokens_path, max_seq_len=512, min_seq_len=384, padding_token=tokenizer['PAD_None'])\n",
    "subset_train, subset_valid = create_subsets(dataset, [0.3])\n",
    "dataloader_train = DataLoader(subset_train, batch_size=16, collate_fn=collate_gen)\n",
    "dataloader_valid = DataLoader(subset_valid, batch_size=16, collate_fn=collate_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "We will use the [GPT2 implementation of Hugging Face](https://huggingface.co/docs/transformers/model_doc/gpt2). This \n",
    "Feel free to explore the documentation and source code to dig deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(GPT2LMHeadModel):\n",
    "    def __init__(self, config: GPT2Config, padding_token: int):\n",
    "        super().__init__(config)\n",
    "        self.transformer.wpe.padding_idx = padding_token  # updates the padding idx\n",
    "        self.transformer.wte.padding_idx = padding_token\n",
    "\n",
    "        self.register_buffer('padding_token', LongTensor([padding_token]))\n",
    "\n",
    "    def forward_train(self, x: LongTensor, target: LongTensor, criterion: Module):\n",
    "        y = self.forward(x).logits  # (N,T,C)\n",
    "        loss = criterion(y.transpose(2, 1), target)\n",
    "        return y, loss, None  # no need for sampled\n",
    "\n",
    "    @no_grad()\n",
    "    def generate(self, x: LongTensor, nb_steps: int, max_seq_len: int, sampling_func: Callable = None) -> LongTensor:\n",
    "        r\"\"\"Generate (extend) from the generator\n",
    "        :param x: input tensor to extend, shape (N,T) or (T)\n",
    "        :param nb_steps: number of steps (inferences) to run\n",
    "        :param max_seq_len: maximum sequence length during inference\n",
    "        :param sampling_func: sampling function (default: top_k with k=15)\n",
    "        :return: the generated tensor\n",
    "        \"\"\"\n",
    "        assert max_seq_len <= (nb_pos := self.transformer.wpe.weight.shape[0]), \\\n",
    "            'The maximum sequence length must be <= to the nb of positions the model can handle'\n",
    "        sampling_func = partial(nucleus, p=0.9) if sampling_func is None else sampling_func\n",
    "        y = x.clone()\n",
    "        if y.dim() == 1:\n",
    "            y = y[x != self.padding_token].unsqueeze(0)  # (T) --> (N,T) with N=1\n",
    "        \n",
    "        # past_key_val stores the past computations so that we do not recompute them again\n",
    "        past_key_val, pos_ids = None, None  # (NLY,2,N,NH,T,DH) & (T'), T' for the non-past-kv part (often 1)\n",
    "        offset = 0\n",
    "        tokens = y.clone()  # (N,T)\n",
    "        for _ in range(nb_steps):\n",
    "            # Adds the prediction to the target sequence, updates past key values and y sequence\n",
    "            logits = self.forward(tokens, past_key_val, position_ids=pos_ids)\n",
    "            logits, past_key_val = logits.logits, logits.past_key_values  # (N,T,C)\n",
    "            tokens = sampling_func(logits[:, -1]).unsqueeze(1).to(x.device)  # (N,1)\n",
    "            y = cat([y, tokens], dim=1)  # (N,T+1)\n",
    "\n",
    "            # Reset past_kv and offset to not exceed pos enc\n",
    "            if past_key_val[0][0].shape[-2] + offset >= nb_pos:\n",
    "                past_key_val, pos_ids, offset = None, None, 0\n",
    "                tokens = y[..., -x.shape[-1]:].clone()  # starting back with len of x for prompt\n",
    "\n",
    "            # Reduces past_kv if the max len is reached\n",
    "            if past_key_val is not None and past_key_val[0][0].shape[-2] >= max_seq_len:\n",
    "                offset += 1\n",
    "                past_key_val = convert_past_key_values_to_tensor(past_key_val)[..., -max_seq_len:, :]\n",
    "                pos_ids = LongTensor([past_key_val.shape[-2] + offset]).to(x.device)\n",
    "\n",
    "        return y[0] if x.dim() == 1 else y  # (T) or (N,T)\n",
    "\n",
    "\n",
    "def convert_past_key_values_to_tensor(past_kv: Tuple) -> Tensor:\n",
    "    \"\"\"Convert past_key_values returned by HF model from tuple(tuple(Tensor)) to a Tensor.\n",
    "    :param past_kv: tuple of past_key_val, shape (NLY,2,N,NH,T,DH) with first two dims as tuple\n",
    "    :return: Tensor of shape (NLY,2,N,NH,T,DH)\n",
    "    \"\"\"\n",
    "    return stack([stack([kv for kv in layer]) for layer in past_kv])  # (NLY,2,N,NH,T,DH)\n",
    "\n",
    "\n",
    "# Creates model\n",
    "config = GPT2Config(vocab_size=len(tokenizer), n_positions=2048, n_embd=512, n_layer=8, n_head=8,\n",
    "                    n_inner=2048, resid_pdrop=.1, embd_pdrop=.1, attn_pdrop=.1,\n",
    "                    bos_token_id=tokenizer['SOS_None'], eos_token_id=tokenizer['EOS_None'])\n",
    "model = Transformer(config, padding_token=tokenizer['PAD_None'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path('run')\n",
    "device = select_device(True)\n",
    "model = model.to(device)\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(params=model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "lr_scheduler = CosineAnnealingWarmRestarts(optimizer, 20, 2)\n",
    "\n",
    "log_model_parameters(model)\n",
    "if device.type == 'cuda':\n",
    "      log_cuda_info()\n",
    "\n",
    "train(model, criterion, optimizer, dataloader_train, dataloader_valid, 50000, 20, 10, None,\n",
    "      'TRAINING MIDI GENERATOR', None, 10, None, lr_scheduler=lr_scheduler, device_=device,\n",
    "      use_amp=True, saving_dir=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OaNkGcFo9UP_"
   },
   "outputs": [],
   "source": [
    "nb_inferences = 512  # extends samples by 512 tokens\n",
    "gen_results_path = Path('gen_res')\n",
    "\n",
    "for i, sample in enumerate(subset_valid):\n",
    "    seq = sample[sample != tokenizer['PAD_None']]  # trims the sample if padded\n",
    "\n",
    "    res = model.generate(seq, nb_inferences, 512)  # generates auto regressively\n",
    "    continuation = res[len(seq):]  # just generated tokens\n",
    "    tokens = [continuation, seq, res]\n",
    "    tokens = [seq.tolist() for seq in tokens]  # converts to lists\n",
    "    midi = tokenizer.tokens_to_midi(deepcopy(tokens), time_division=384)\n",
    "    midi.instruments[0].name = f'Continuation of original sample ({len(continuation)} tokens)'\n",
    "    midi.instruments[1].name = f'Original sample ({len(seq)} tokens)'\n",
    "    midi.instruments[2].name = f'Original sample and continuation'\n",
    "    midi.dump(gen_results_path / f'{i}.mid')\n",
    "    tokenizer.save_tokens(tokens, gen_results_path / f'{i}.json')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Optimus_VIRTUOSO_Multi_Instrumental_RGA_Edition.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
